{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T01:12:39.862266Z",
     "start_time": "2020-12-10T01:12:37.084961Z"
    },
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T01:13:22.804571Z",
     "start_time": "2020-12-10T01:13:22.137882Z"
    }
   },
   "outputs": [],
   "source": [
    "import dedupe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from casamento_entidade.diversos import carregar_json, criar_dir\n",
    "from casamento_entidade.entrada import df_para_dict\n",
    "from casamento_entidade.modelos_ce import ModeloDedupe\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T01:13:23.339369Z",
     "start_time": "2020-12-10T01:13:23.335585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logs\n",
    "arquivo_logging = 'saidas/execucao.log'\n",
    "log_level = 'INFO'\n",
    "\n",
    "import logging\n",
    "logging.getLogger('dedupe').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('casamento_entidade').setLevel(logging.CRITICAL)\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(filename=arquivo_logging, level=log_level, filemode='w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T01:13:27.015411Z",
     "start_time": "2020-12-10T01:13:27.007178Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Arquivos de entrada/saida\n",
    "arquivo_modelo_treino = Path(f'saidas/arquivo_treinamento.json')\n",
    "arquivo_modelo_configuracoes = Path(f'saidas/arquivo_configuracao')\n",
    "\n",
    "arquivo_dados_treino = 'dados_treino/dados_pessoa_treino_amostra.csv'\n",
    "\n",
    "# Configuracoes dedupe\n",
    "casamento_selecionado = 'proba'\n",
    "\n",
    "tipos_casamento = {'exato': 'Exact', 'proba': 'String'}\n",
    "campos_casamento = [\n",
    "    {\n",
    "        'field': 'num_cpf_cnpj',\n",
    "        'type': tipos_casamento[casamento_selecionado],\n",
    "        'has missing': True\n",
    "    },\n",
    "    {\n",
    "        'field': 'nome_pessoa',\n",
    "        'type': tipos_casamento[casamento_selecionado],\n",
    "        'has missing': True\n",
    "    },\n",
    "    {\n",
    "        'field': 'num_titulo_eleitor',\n",
    "        'type': tipos_casamento[casamento_selecionado],\n",
    "        'has missing': True\n",
    "    },\n",
    "    {\n",
    "        'field': 'num_nis_nit',\n",
    "        'type': tipos_casamento[casamento_selecionado],\n",
    "        'has missing': True\n",
    "    },\n",
    "    {\n",
    "        'field': 'num_cliente_cemig',\n",
    "        'type': tipos_casamento[casamento_selecionado],\n",
    "        'has missing': True\n",
    "    },\n",
    "    {\n",
    "        'field': 'nome_pai',\n",
    "        'type': tipos_casamento[casamento_selecionado],\n",
    "        'has missing': True\n",
    "    },\n",
    "    {\n",
    "        'field': 'nome_mae',\n",
    "        'type': tipos_casamento[casamento_selecionado],\n",
    "        'has missing': True\n",
    "    },\n",
    "    {\n",
    "        'field': 'data_nasc',\n",
    "        'type': tipos_casamento[casamento_selecionado],\n",
    "        'has missing': True\n",
    "    }\n",
    "]\n",
    "\n",
    "num_cores = 15\n",
    "forcar_treinamento = False\n",
    "sample_size = 15_000\n",
    "blocked_proportion = 0.9\n",
    "original_length = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento do Dedupe para **Casamento de Entidades** utilizando dados de treino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrada do Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T01:13:29.404068Z",
     "start_time": "2020-12-10T01:13:29.221399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b70a1392b784794b07217d24b987ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Leitura\n",
    "colunas_treino = ['uuid_pessoa', 'num_cpf_cnpj', 'nome_pessoa', 'data_nasc', \n",
    "                  'nome_mae', 'nome_pai', 'nit', 'nis', 'num_titulo_eleitor', 'num_cliente_cemig']\n",
    "colunas_idx = [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Colunas para ler do csv\n",
    "df_treino = pd.read_csv(arquivo_dados_treino, low_memory=False, sep='#', header=None, usecols=colunas_idx)\n",
    "df_treino.columns = colunas_treino\n",
    "\n",
    "# Coluna Nis/Nit\n",
    "df_treino['num_nis_nit'] = df_treino['nis'].combine_first(df_treino['nit'])\n",
    "df_treino.drop(['nis', 'nit'], inplace=True, axis=1)\n",
    "\n",
    "# Removendo uuid duplicados\n",
    "df_treino.sort_values(by='num_cpf_cnpj', inplace=True, ignore_index=True)\n",
    "df_treino.drop_duplicates(subset=['uuid_pessoa'], keep='first', inplace=True, \n",
    "                          ignore_index=True)\n",
    "\n",
    "# UUID como indice\n",
    "df_treino.set_index('uuid_pessoa', inplace=True)\n",
    "\n",
    "# Conversao para str se probabilistico\n",
    "if casamento_selecionado == 'proba': \n",
    "    for c in tqdm(df_treino.columns):\n",
    "        df_treino = df_treino.astype(str)\n",
    "\n",
    "# Entrada para dedupe\n",
    "df_treino = df_treino.where(pd.notnull(df_treino), None)  # dedupe soh aceita None\n",
    "df_treino.reset_index(drop=True, inplace=True)\n",
    "dados_dedupe_treino = df_treino.to_dict(orient='index')  # dedupe soh aceita dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento com Aprendizado Ativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T01:15:21.564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando o treinamento do modelo...\n"
     ]
    }
   ],
   "source": [
    "# Configurações Dedupe\n",
    "if (arquivo_modelo_configuracoes.exists()) and (not forcar_treinamento):\n",
    "    print(f'Carregando o modelo a partir do arquivo de configuração {arquivo_modelo_configuracoes}')\n",
    "    with arquivo_modelo_configuracoes.open(mode='rb') as fp:\n",
    "        modelo_ce = dedupe.StaticDedupe(settings_file=fp, num_cores=num_cores)\n",
    "else:\n",
    "    # Instanciação\n",
    "    modelo_ce = dedupe.Dedupe(variable_definition=campos_casamento, num_cores=num_cores)\n",
    "\n",
    "    # Preparando treino\n",
    "    print('Preparando o treinamento do modelo...')\n",
    "    if arquivo_modelo_treino.exists():\n",
    "        print(f'\\tLendo exemplos do arquivo de treinamento ({arquivo_modelo_treino}).')\n",
    "        with arquivo_modelo_treino.open() as fp:\n",
    "            modelo_ce.prepare_training(data=dados_dedupe_treino, training_file=fp,\n",
    "                                       sample_size=sample_size, blocked_proportion=blocked_proportion,\n",
    "                                       original_length=original_length)\n",
    "    else:\n",
    "        modelo_ce.prepare_training(data=dados_dedupe_treino, training_file=None,\n",
    "                                   sample_size=sample_size, blocked_proportion=blocked_proportion,\n",
    "                                   original_length=original_length)\n",
    "\n",
    "    # Active Learning\n",
    "    print('Iniciando o treinamento com Active Learning...')\n",
    "    dedupe.console_label(modelo_ce)\n",
    "    modelo_ce.train()\n",
    "\n",
    "    # Salva configuração e exemplos de treino\n",
    "    print('Salvando os arquivos de treino e configuração...')\n",
    "    with arquivo_modelo_treino.open(mode='w+') as fp:\n",
    "        modelo_ce.write_training(file_obj=fp)\n",
    "\n",
    "    with arquivo_config.open(mode='wb+') as fp:\n",
    "        modelo_ce.write_settings(file_obj=fp)\n",
    "\n",
    "    # Liberando memória\n",
    "    modelo_ce.cleanup_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
