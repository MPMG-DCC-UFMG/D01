{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dedupe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from casamento_entidade.diversos import carregar_json, criar_dir\n",
    "from casamento_entidade.entrada import df_para_dict\n",
    "from casamento_entidade.modelos_ce import ModeloDedupe\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"dedupe\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"casamento_entidade\").setLevel(logging.CRITICAL)\n",
    "\n",
    "arquivo_logging = \"\"\"../versao_01/output/execution.log\"\"\"\n",
    "log_level = \"INFO\"\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(filename=arquivo_logging, \n",
    "                    level=log_level, filemode=\"w+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento do Dedupe para **Casamento de Entidades** utilizando dados de treino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrada do Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura\n",
    "arquivo_treino = ARQUIVO_ENTRADA\n",
    "df_treino = pd.read_csv(arquivo_treino)\n",
    "\n",
    "# uuid como indice\n",
    "# Deve haver um ID único para identificar os registros\n",
    "df_treino.set_index(\"uuid\", inplace=True)\n",
    "\n",
    "# Entrada para dedupe\n",
    "df_treino = df_treino.where(pd.notnull(df_treino), None)  # dedupe soh aceita None\n",
    "df_treino.reset_index(drop=True, inplace=True)\n",
    "dados_dedupe_treino = df_treino.to_dict(orient=\"index\")  # dedupe soh aceita dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento com Aprendizado Ativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo_treino = Path(f\"\"\"../versao_vm/saidas/arquivo_treinamento.json\"\"\")\n",
    "arquivo_config = Path(f\"\"\"../versao_vm/saidas/arquivo_configuracao\"\"\")\n",
    "\n",
    "# Configurações Dedupe\n",
    "campos_casamento = [\n",
    "    {\n",
    "        \"field\": CAMPO_CASAMENTO,\n",
    "        \"type\": \"Exact\",\n",
    "        \"has missing\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "num_cores = 32\n",
    "forcar_treinamento = False\n",
    "sample_size = 15_000\n",
    "blocked_proportion = 0.9\n",
    "original_length = None\n",
    "\n",
    "if (arquivo_config.exists()) and (not forcar_treinamento):\n",
    "    print(f\"Carregando o modelo a partir do arquivo de configuração {arquivo_config}\")\n",
    "    with arquivo_config.open(mode=\"rb\") as fp:\n",
    "        modelo_ce = dedupe.StaticDedupe(settings_file=fp, num_cores=num_cores)\n",
    "else:\n",
    "    # Instanciação\n",
    "    modelo_ce = dedupe.Dedupe(variable_definition=campos_casamento, num_cores=num_cores)\n",
    "\n",
    "    # Preparando treino\n",
    "    print(\"Preparando o treinamento do modelo...\")\n",
    "    if arquivo_treino.exists():\n",
    "        print(f\"\\tLendo exemplos do arquivo de treinamento ({arquivo_treino}).\")\n",
    "        with arquivo_treino.open() as fp:\n",
    "            modelo_ce.prepare_training(data=dados_dedupe_treino, training_file=fp,\n",
    "                                       sample_size=sample_size,\n",
    "                                       blocked_proportion=blocked_proportion,\n",
    "                                       original_length=original_length)\n",
    "    else:\n",
    "        modelo_ce.prepare_training(data=dados_dedupe_treino, training_file=None,\n",
    "                                   sample_size=sample_size,\n",
    "                                   blocked_proportion=blocked_proportion,\n",
    "                                   original_length=original_length)\n",
    "\n",
    "    # Active Learning\n",
    "    print(\"Iniciando o treinamento com Active Learning...\")\n",
    "    dedupe.console_label(modelo_ce)\n",
    "    modelo_ce.train()\n",
    "\n",
    "    # Salva configuração e exemplos de treino\n",
    "    print(\"Salvando os arquivos de treino e configuração...\")\n",
    "    with arquivo_treino.open(mode=\"w+\") as fp:\n",
    "        modelo_ce.write_training(file_obj=fp)\n",
    "\n",
    "    with arquivo_config.open(mode=\"wb+\") as fp:\n",
    "        modelo_ce.write_settings(file_obj=fp)\n",
    "\n",
    "    # Liberando memória\n",
    "    modelo_ce.cleanup_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casamento com Base de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo dados de teste\n",
    "arquivo_teste = ARQUIVO_TESTE\n",
    "df_ce = pd.read_csv(arquivo_teste)\n",
    "\n",
    "# Removendo uuid duplicados\n",
    "df_ce.drop_duplicates(subset=[\"uuid\"], keep=\"first\", inplace=True, \n",
    "                      ignore_index=True)\n",
    "df_ce.set_index(\"uuid\", inplace=True)\n",
    "\n",
    "# Entrada para dedupe\n",
    "df_ce = df_ce.where(pd.notnull(df_ce), None)  # dedupe soh aceita None\n",
    "df_ce.reset_index(drop=True, inplace=True)\n",
    "dados_dedupe_ce = df_ce.to_dict(orient=\"index\")  # dedupe soh aceita dict\n",
    "del df_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando modelo treinado\n",
    "arquivo_config = Path(f\"\"\"../versao_01/output/arquivo_configuracao\"\"\")\n",
    "num_cores = 32\n",
    "\n",
    "with arquivo_config.open(mode=\"rb\") as fp:\n",
    "    modelo_ce = dedupe.StaticDedupe(settings_file=fp, num_cores=num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração dos grupos\n",
    "hreshold = 0.5\n",
    "grupos = modelo_ce.partition(data=dados_dedupe_ce, threshold=threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
